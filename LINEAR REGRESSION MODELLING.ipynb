{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from statsmodels.formula.api import ols\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_data = pd.read_csv('final_players.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_name</th>\n",
       "      <th>age</th>\n",
       "      <th>league</th>\n",
       "      <th>season</th>\n",
       "      <th>position</th>\n",
       "      <th>current_club</th>\n",
       "      <th>minutes_played_overall</th>\n",
       "      <th>minutes_played_home</th>\n",
       "      <th>minutes_played_away</th>\n",
       "      <th>nationality</th>\n",
       "      <th>...</th>\n",
       "      <th>min_per_match</th>\n",
       "      <th>min_per_card_overall</th>\n",
       "      <th>min_per_assist_overall</th>\n",
       "      <th>cards_per_90_overall</th>\n",
       "      <th>rank_in_league_top_attackers</th>\n",
       "      <th>rank_in_league_top_midfielders</th>\n",
       "      <th>rank_in_league_top_defenders</th>\n",
       "      <th>rank_in_club_top_scorer</th>\n",
       "      <th>weekly_wages</th>\n",
       "      <th>market_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>De Gea</td>\n",
       "      <td>29</td>\n",
       "      <td>Premier League</td>\n",
       "      <td>2018/2019</td>\n",
       "      <td>Goalkeeper</td>\n",
       "      <td>Manchester United</td>\n",
       "      <td>3420</td>\n",
       "      <td>1710</td>\n",
       "      <td>1710</td>\n",
       "      <td>Spain</td>\n",
       "      <td>...</td>\n",
       "      <td>90</td>\n",
       "      <td>3420</td>\n",
       "      <td>0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>310</td>\n",
       "      <td>419</td>\n",
       "      <td>90</td>\n",
       "      <td>18</td>\n",
       "      <td>200000</td>\n",
       "      <td>56000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Matteo Darmian</td>\n",
       "      <td>30</td>\n",
       "      <td>Premier League</td>\n",
       "      <td>2018/2019</td>\n",
       "      <td>Defender</td>\n",
       "      <td>Manchester United</td>\n",
       "      <td>443</td>\n",
       "      <td>353</td>\n",
       "      <td>90</td>\n",
       "      <td>Italy</td>\n",
       "      <td>...</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>411</td>\n",
       "      <td>366</td>\n",
       "      <td>102</td>\n",
       "      <td>25</td>\n",
       "      <td>36000</td>\n",
       "      <td>5500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Victor Nilsson Lindelöf</td>\n",
       "      <td>25</td>\n",
       "      <td>Premier League</td>\n",
       "      <td>2018/2019</td>\n",
       "      <td>Defender</td>\n",
       "      <td>Manchester United</td>\n",
       "      <td>2602</td>\n",
       "      <td>1112</td>\n",
       "      <td>1490</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>...</td>\n",
       "      <td>87</td>\n",
       "      <td>2602</td>\n",
       "      <td>2602</td>\n",
       "      <td>0.03</td>\n",
       "      <td>263</td>\n",
       "      <td>249</td>\n",
       "      <td>88</td>\n",
       "      <td>14</td>\n",
       "      <td>97000</td>\n",
       "      <td>22000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Luke Shaw</td>\n",
       "      <td>24</td>\n",
       "      <td>Premier League</td>\n",
       "      <td>2018/2019</td>\n",
       "      <td>Defender</td>\n",
       "      <td>Manchester United</td>\n",
       "      <td>2592</td>\n",
       "      <td>1170</td>\n",
       "      <td>1422</td>\n",
       "      <td>England</td>\n",
       "      <td>...</td>\n",
       "      <td>89</td>\n",
       "      <td>236</td>\n",
       "      <td>648</td>\n",
       "      <td>0.38</td>\n",
       "      <td>262</td>\n",
       "      <td>104</td>\n",
       "      <td>79</td>\n",
       "      <td>13</td>\n",
       "      <td>90000</td>\n",
       "      <td>16500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Eric Bertrand Bailly</td>\n",
       "      <td>25</td>\n",
       "      <td>Premier League</td>\n",
       "      <td>2018/2019</td>\n",
       "      <td>Defender</td>\n",
       "      <td>Manchester United</td>\n",
       "      <td>637</td>\n",
       "      <td>349</td>\n",
       "      <td>288</td>\n",
       "      <td>Côte d'Ivoire</td>\n",
       "      <td>...</td>\n",
       "      <td>53</td>\n",
       "      <td>319</td>\n",
       "      <td>0</td>\n",
       "      <td>0.28</td>\n",
       "      <td>340</td>\n",
       "      <td>271</td>\n",
       "      <td>142</td>\n",
       "      <td>26</td>\n",
       "      <td>100000</td>\n",
       "      <td>16500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 full_name  age          league     season    position  \\\n",
       "0                   De Gea   29  Premier League  2018/2019  Goalkeeper   \n",
       "1           Matteo Darmian   30  Premier League  2018/2019    Defender   \n",
       "2  Victor Nilsson Lindelöf   25  Premier League  2018/2019    Defender   \n",
       "3                Luke Shaw   24  Premier League  2018/2019    Defender   \n",
       "4     Eric Bertrand Bailly   25  Premier League  2018/2019    Defender   \n",
       "\n",
       "        current_club  minutes_played_overall  minutes_played_home  \\\n",
       "0  Manchester United                    3420                 1710   \n",
       "1  Manchester United                     443                  353   \n",
       "2  Manchester United                    2602                 1112   \n",
       "3  Manchester United                    2592                 1170   \n",
       "4  Manchester United                     637                  349   \n",
       "\n",
       "   minutes_played_away    nationality  ...  min_per_match  \\\n",
       "0                 1710          Spain  ...             90   \n",
       "1                   90          Italy  ...             74   \n",
       "2                 1490         Sweden  ...             87   \n",
       "3                 1422        England  ...             89   \n",
       "4                  288  Côte d'Ivoire  ...             53   \n",
       "\n",
       "   min_per_card_overall  min_per_assist_overall  cards_per_90_overall  \\\n",
       "0                  3420                       0                  0.03   \n",
       "1                     0                       0                  0.00   \n",
       "2                  2602                    2602                  0.03   \n",
       "3                   236                     648                  0.38   \n",
       "4                   319                       0                  0.28   \n",
       "\n",
       "   rank_in_league_top_attackers  rank_in_league_top_midfielders  \\\n",
       "0                           310                             419   \n",
       "1                           411                             366   \n",
       "2                           263                             249   \n",
       "3                           262                             104   \n",
       "4                           340                             271   \n",
       "\n",
       "   rank_in_league_top_defenders  rank_in_club_top_scorer  weekly_wages  \\\n",
       "0                            90                       18        200000   \n",
       "1                           102                       25         36000   \n",
       "2                            88                       14         97000   \n",
       "3                            79                       13         90000   \n",
       "4                           142                       26        100000   \n",
       "\n",
       "   market_value  \n",
       "0      56000000  \n",
       "1       5500000  \n",
       "2      22000000  \n",
       "3      16500000  \n",
       "4      16500000  \n",
       "\n",
       "[5 rows x 47 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  USING DOMAIN KNOWLEDGE FOR FEATURE SELECTION AND BUILD BASELINE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = reg_data['weekly_wages']\n",
    "predictors = reg_data[['age', 'assists_overall', 'penalty_goals', 'penalty_misses', \n",
    "               'minutes_played_overall','market_value', 'clean_sheets_overall']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(predictors, target, random_state = 42, test_size = 0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "X_train_scaled  = ss.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linreg = LinearRegression()\n",
    "linreg.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline model for Continuous variables\n",
      "Training r^2: 0.67336666045365\n",
      "Testing r^2: 0.6441085449289107\n",
      "Training MSE: 479954644.88109446\n",
      "Testing MSE: 508515576.75408506\n"
     ]
    }
   ],
   "source": [
    "print('Baseline model for Continuous variables')\n",
    "print('Training r^2:', linreg.score(X_train_scaled, y_train))\n",
    "print('Testing r^2:', linreg.score(X_test_scaled, y_test))\n",
    "print('Training MSE:', mean_squared_error(y_train, linreg.predict(X_train_scaled)))\n",
    "print('Testing MSE:', mean_squared_error(y_test, linreg.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above model is performing averagely but take into account we haven't added the categorical variables and the difference in MSE between training and the test is not a lot so the model points more towards underfitting than overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying log-transform before scaling the features but first things first we check for the normality of the distribution of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_log = np.log(X_train + 1)\n",
    "X_test_log = np.log(X_test + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_X_train_scaled = ss.fit_transform(X_train_log)\n",
    "log_X_test_scaled = ss.transform(X_test_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linreg_2 = LinearRegression()\n",
    "linreg_2.fit(log_X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline model for Continuous variables\n",
      "Training r^2: 0.3510315129418742\n",
      "Testing r^2: 0.36215117177233347\n",
      "Training MSE: 953593531.442938\n",
      "Testing MSE: 911390425.778329\n"
     ]
    }
   ],
   "source": [
    "print('Baseline model for Continuous variables')\n",
    "print('Training r^2:', linreg_2.score(log_X_train_scaled, y_train))\n",
    "print('Testing r^2:', linreg_2.score(log_X_test_scaled, y_test))\n",
    "print('Training MSE:', mean_squared_error(y_train, linreg_2.predict(log_X_train_scaled)))\n",
    "print('Testing MSE:', mean_squared_error(y_test, linreg_2.predict(log_X_test_scaled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the log transformations of the features, the model gets worse as the proportion of varaince explained by the model decreases when compared to the first model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to add the categorical variables, the league and the position of each player into the model features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using One Hot Encoder\n",
    "cat_predictors = reg_data[['league', 'position']]\n",
    "X_train_cat, X_test_cat, y_train, y_test = train_test_split(cat_predictors, target, test_size = 0.30, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categories='auto', drop=None, dtype=<class 'numpy.float64'>,\n",
       "              handle_unknown='ignore', sparse=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "ohe.fit(X_train_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ohe = ohe.transform(X_train_cat)\n",
    "X_test_ohe = ohe.transform(X_test_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ohe.get_feature_names(input_features=X_train_cat.columns)\n",
    "cat_train_df = pd.DataFrame(X_train_ohe.todense(), columns=columns)\n",
    "cat_test_df = pd.DataFrame(X_test_ohe.todense(), columns=columns)\n",
    "X_train_all = pd.concat([pd.DataFrame(X_train_scaled), cat_train_df], axis = 1)\n",
    "X_test_all = pd.concat([pd.DataFrame(X_test_scaled), cat_test_df], axis = 1)\n",
    "linreg_all = LinearRegression()\n",
    "linreg_all.fit(X_train_all, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline model Continuous and Categorical\n",
      "Training r^2: 0.6885733234410595\n",
      "Testing r^2: 0.6724766647520386\n",
      "Training MSE: 457609992.176367\n",
      "Testing MSE: 467981783.07138664\n"
     ]
    }
   ],
   "source": [
    "print('Baseline model Continuous and Categorical')\n",
    "print('Training r^2:', linreg_all.score(X_train_all, y_train))\n",
    "print('Testing r^2:', linreg_all.score(X_test_all, y_test))\n",
    "print('Training MSE:', mean_squared_error(y_train, linreg_all.predict(X_train_all)))\n",
    "print('Testing MSE:', mean_squared_error(y_test, linreg_all.predict(X_test_all)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with the adding of categorical variables, the R-squared of the training and test data improved and the MSE also decreased and so did the difference in the Training and the testing model MSE. which means this model is more generalizable and more robust than the previous two models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POLYNOMIAL REGRESSION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accounting for interactions in a model is quite important, what you are essentially doing is transforming a variable role in a linear regression based on another one, using PolyNomial features allows both interactions and polynomial expansions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_features = PolynomialFeatures(2)\n",
    "  \n",
    "  # transforms the existing features to higher degree features.\n",
    "X_train_poly = poly_features.fit_transform(X_train_scaled)\n",
    "\n",
    "# fit the transformed features to Linear Regression\n",
    "poly_model = LinearRegression()\n",
    "poly_model.fit(X_train_poly, y_train)\n",
    "\n",
    "# predicting on training data-set\n",
    "y_train_predicted = poly_model.predict(X_train_poly)\n",
    "\n",
    "# predicting on test data-set\n",
    "y_test_predict = poly_model.predict(poly_features.fit_transform(X_test_scaled))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynomial expansion systematically provides automatic means of creating both interactions and non-linear power transformations of the original variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Polynomial training set for continuous\n",
      "MSE of training set is 19773.982259392626\n",
      "R2 score of training set is 0.7338977219035463\n",
      "\n",
      "\n",
      "Polynomial test set for continuous\n",
      "MSE of test set is 20622.378317273862\n",
      "R2 score of test set is 0.7023603401791982\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluating the model on training dataset\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train, y_train_predicted))\n",
    "r2_train = r2_score(y_train, y_train_predicted)\n",
    "\n",
    "# evaluating the model on test dataset\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_test_predict))\n",
    "r2_test = r2_score(y_test, y_test_predict)\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\" Polynomial training set for continuous\")\n",
    "\n",
    "print(\"MSE of training set is {}\".format(rmse_train))\n",
    "print(\"R2 score of training set is {}\".format(r2_train))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Polynomial test set for continuous\")\n",
    "\n",
    "print(\"MSE of test set is {}\".format(rmse_test))\n",
    "print(\"R2 score of test set is {}\".format(r2_test))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are able to get an improved R-squared of 0.702 for the test set, but the main question is does the model overfit? The difference between the RMSE of both training set predictions and test set predictions is not that high. So at the moment the model does not overfit to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Polynomial training set for continuous and categorical\n",
      "MSE of training set is 17300.084685664737\n",
      "R2 score of training set is 0.7963160679576795\n",
      "\n",
      "\n",
      "Polynomial test set for continuous and categorical \n",
      "MSE of test set is 19779.01250959649\n",
      "R2 score of test set is 0.7262068928872164\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "poly_features = PolynomialFeatures(2)\n",
    "  \n",
    "  # transforms the existing features to higher degree features.\n",
    "X_train_poly = poly_features.fit_transform(X_train_all)\n",
    "\n",
    "# fit the transformed features to Linear Regression\n",
    "poly_model = LinearRegression()\n",
    "poly_model.fit(X_train_poly, y_train)\n",
    "\n",
    "# predicting on training data-set\n",
    "y_train_predicted = poly_model.predict(X_train_poly)\n",
    "\n",
    "# predicting on test data-set\n",
    "y_test_predict = poly_model.predict(poly_features.fit_transform(X_test_all))\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train, y_train_predicted))\n",
    "r2_train = r2_score(y_train, y_train_predicted)\n",
    "\n",
    "# evaluating the model on test dataset\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_test_predict))\n",
    "r2_test = r2_score(y_test, y_test_predict)\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\" Polynomial training set for continuous and categorical\")\n",
    "\n",
    "print(\"MSE of training set is {}\".format(rmse_train))\n",
    "print(\"R2 score of training set is {}\".format(r2_train))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Polynomial test set for continuous and categorical \")\n",
    "\n",
    "print(\"MSE of test set is {}\".format(rmse_test))\n",
    "print(\"R2 score of test set is {}\".format(r2_test))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result above constitutes the best prediction at r-squared = 0.72 for the test set with both continuous and categorical features taken into account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing to test out is whether interactions between two features can get a better predicting model than the baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top interactions: [('age', 'market_value', 0.692), ('age', 'assists_overall', 0.662)]\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "combinations = list(combinations(X_train.columns, 2))\n",
    "interactions = []\n",
    "crossvalidation = KFold(n_splits=3, shuffle=True, random_state=1)\n",
    "baseline = np.mean(cross_val_score(linreg,predictors, target, scoring='r2', cv=crossvalidation))\n",
    "\n",
    "data = X_train.copy()\n",
    "for comb in combinations:\n",
    "    data['interaction'] = data[comb[0]] * data[comb[1]]\n",
    "    score = np.mean(cross_val_score(linreg, data, y_train, scoring='r2', cv=crossvalidation))\n",
    "    if score > baseline: interactions.append((comb[0], comb[1], round(score,3)))\n",
    "            \n",
    "print('Top interactions: %s' %sorted(interactions, key=lambda inter: inter[2], reverse=True)[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it turns out, multiplying age and market value as one predictor variable will give a better predicting model of 0.692"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running a cross validation on the train polynomial fitted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Mean r2: -1.3961612169908437e+19\n",
      "Cross Validation Mean MSE: -3.717660535344389e+28\n",
      "Cross Validation 10 Fold Score: [ 5.36122899e-01  7.10239682e-01  6.47703873e-01  6.99841520e-01\n",
      "  7.46730447e-01 -1.39616122e+20  7.10631993e-01  4.72138545e-01\n",
      "  8.28664951e-01  6.44435068e-01]\n",
      "Cross Validation 10 Fold mean squared error [3.32116214e+08 3.18903548e+08 2.77901354e+08 5.09916868e+08\n",
      " 3.54793675e+08 3.71766054e+29 2.51313166e+08 4.21364558e+08\n",
      " 5.37120488e+08 5.14131125e+08]\n"
     ]
    }
   ],
   "source": [
    "lm = LinearRegression()\n",
    "\n",
    "scores = cross_val_score(lm, X_train_poly, y_train, cv=10, scoring='r2')\n",
    "mse_scores = cross_val_score(lm, X_train_poly, y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "\n",
    "print('Cross Validation Mean r2:',np.mean(scores))\n",
    "print('Cross Validation Mean MSE:',np.mean(mse_scores))\n",
    "print('Cross Validation 10 Fold Score:',scores)\n",
    "print ('Cross Validation 10 Fold mean squared error',-(mse_scores) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation mean for the poly model is 0.658"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RIDGE AND LASSO REGULARIZATION TECHNIQUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso\n",
      "Training r^2: 0.6885733204674268\n",
      "Testing r^2: 0.6724757368979337\n",
      "Training MSE: 457609996.54581934\n",
      "Testing MSE: 467983108.83590996\n",
      "\n",
      "\n",
      "Ridge\n",
      "Training r^2: 0.6885733097449265\n",
      "Testing r^2: 0.6724810119135127\n",
      "Training MSE: 457610012.30144894\n",
      "Testing MSE: 467975571.628845\n"
     ]
    }
   ],
   "source": [
    "lasso = Lasso(alpha=0.2) #Lasso is also known as the L1 norm.\n",
    "lasso.fit(X_train_all, y_train)\n",
    "print( 'Lasso')\n",
    "print('Training r^2:', lasso.score(X_train_all, y_train))\n",
    "print('Testing r^2:', lasso.score(X_test_all, y_test))\n",
    "print('Training MSE:', mean_squared_error(y_train, lasso.predict(X_train_all)))\n",
    "print('Testing MSE:', mean_squared_error(y_test, lasso.predict(X_test_all)))\n",
    "\n",
    "print(\"\\n\")\n",
    "    \n",
    "ridge = Ridge(alpha = 0.2) #Ridge is also known as the L2 norm.\n",
    "ridge.fit(X_train_all, y_train)\n",
    "print('Ridge')\n",
    "print('Training r^2:', ridge.score(X_train_all, y_train))\n",
    "print('Testing r^2:', ridge.score(X_test_all, y_test))\n",
    "print('Training MSE:', mean_squared_error(y_train, ridge.predict(X_train_all)))\n",
    "print('Testing MSE:', mean_squared_error(y_test, ridge.predict(X_test_all)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the Ridge and Lasso Regqularization is used whne avoiding the problem of model complexity a sthe model won't learn from general rules but will just be memeorizing the dataset itself.\n",
    "The lasso regularization technique does a good job of feature selection and parameter shrinkage whilst ridge only shrinks coefficients of parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "our best model so far is the PolyRegression model, we are going to loop through a Poly features of 10 degrees to see which gives the lowest RMSE score and the best model based on all the predictors selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best degree 2 with RMSE 19781.8849302735\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'RMSE')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rmses = []\n",
    "degrees = np.arange(1, 5)\n",
    "min_rmse, min_deg = 1e10, 0\n",
    "for deg in degrees:\n",
    "\n",
    "    # Train features\n",
    "    poly_features = PolynomialFeatures(degree=deg, include_bias=False)\n",
    "    x_poly_train = poly_features.fit_transform(X_train_all)\n",
    "\n",
    "    # Linear regression\n",
    "    poly_reg = LinearRegression()\n",
    "    poly_reg.fit(x_poly_train, y_train)\n",
    "\n",
    "    # Compare with test data\n",
    "    x_poly_test = poly_features.fit_transform(X_test_all)\n",
    "    poly_predict = poly_reg.predict(x_poly_test)\n",
    "    poly_mse = mean_squared_error(y_test, poly_predict)\n",
    "    poly_rmse = np.sqrt(poly_mse)\n",
    "    rmses.append(poly_rmse)\n",
    "\n",
    "    # Cross-validation of degree\n",
    "    if min_rmse > poly_rmse:\n",
    "        min_rmse = poly_rmse\n",
    "        min_deg = deg\n",
    "\n",
    "# Plot and present results\n",
    "print('Best degree {} with RMSE {}'.format(min_deg, min_rmse))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(degrees, rmses)\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('Degree')\n",
    "ax.set_ylabel('RMSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly the best Poly model is with 2 PolyNomial Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USING RIDGE AND LASSO REGULARIZATION TO REDUCE MODEL COMPLEXITY AND ELIMINATE MULTICOLLINEARITY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lass_target = reg_data['weekly_wages']\n",
    "lass_predictor = reg_data.drop(columns = ['weekly_wages', 'full_name','season', 'position', 'current_club',\n",
    "                               'nationality','league' ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>minutes_played_overall</th>\n",
       "      <th>minutes_played_home</th>\n",
       "      <th>minutes_played_away</th>\n",
       "      <th>appearances_overall</th>\n",
       "      <th>appearances_home</th>\n",
       "      <th>appearances_away</th>\n",
       "      <th>goals_overall</th>\n",
       "      <th>goals_home</th>\n",
       "      <th>goals_away</th>\n",
       "      <th>...</th>\n",
       "      <th>min_per_conceded_overall</th>\n",
       "      <th>min_per_match</th>\n",
       "      <th>min_per_card_overall</th>\n",
       "      <th>min_per_assist_overall</th>\n",
       "      <th>cards_per_90_overall</th>\n",
       "      <th>rank_in_league_top_attackers</th>\n",
       "      <th>rank_in_league_top_midfielders</th>\n",
       "      <th>rank_in_league_top_defenders</th>\n",
       "      <th>rank_in_club_top_scorer</th>\n",
       "      <th>market_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29</td>\n",
       "      <td>3420</td>\n",
       "      <td>1710</td>\n",
       "      <td>1710</td>\n",
       "      <td>38</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>67</td>\n",
       "      <td>90</td>\n",
       "      <td>3420</td>\n",
       "      <td>0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>310</td>\n",
       "      <td>419</td>\n",
       "      <td>90</td>\n",
       "      <td>18</td>\n",
       "      <td>56000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "      <td>443</td>\n",
       "      <td>353</td>\n",
       "      <td>90</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>63</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>411</td>\n",
       "      <td>366</td>\n",
       "      <td>102</td>\n",
       "      <td>25</td>\n",
       "      <td>5500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25</td>\n",
       "      <td>2602</td>\n",
       "      <td>1112</td>\n",
       "      <td>1490</td>\n",
       "      <td>30</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>68</td>\n",
       "      <td>87</td>\n",
       "      <td>2602</td>\n",
       "      <td>2602</td>\n",
       "      <td>0.03</td>\n",
       "      <td>263</td>\n",
       "      <td>249</td>\n",
       "      <td>88</td>\n",
       "      <td>14</td>\n",
       "      <td>22000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24</td>\n",
       "      <td>2592</td>\n",
       "      <td>1170</td>\n",
       "      <td>1422</td>\n",
       "      <td>29</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>72</td>\n",
       "      <td>89</td>\n",
       "      <td>236</td>\n",
       "      <td>648</td>\n",
       "      <td>0.38</td>\n",
       "      <td>262</td>\n",
       "      <td>104</td>\n",
       "      <td>79</td>\n",
       "      <td>13</td>\n",
       "      <td>16500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>637</td>\n",
       "      <td>349</td>\n",
       "      <td>288</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>53</td>\n",
       "      <td>53</td>\n",
       "      <td>319</td>\n",
       "      <td>0</td>\n",
       "      <td>0.28</td>\n",
       "      <td>340</td>\n",
       "      <td>271</td>\n",
       "      <td>142</td>\n",
       "      <td>26</td>\n",
       "      <td>16500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  minutes_played_overall  minutes_played_home  minutes_played_away  \\\n",
       "0   29                    3420                 1710                 1710   \n",
       "1   30                     443                  353                   90   \n",
       "2   25                    2602                 1112                 1490   \n",
       "3   24                    2592                 1170                 1422   \n",
       "4   25                     637                  349                  288   \n",
       "\n",
       "   appearances_overall  appearances_home  appearances_away  goals_overall  \\\n",
       "0                   38                19                19              0   \n",
       "1                    6                 4                 1              0   \n",
       "2                   30                12                17              1   \n",
       "3                   29                13                16              1   \n",
       "4                   12                 5                 3              0   \n",
       "\n",
       "   goals_home  goals_away  ...  min_per_conceded_overall  min_per_match  \\\n",
       "0           0           0  ...                        67             90   \n",
       "1           0           0  ...                        63             74   \n",
       "2           1           0  ...                        68             87   \n",
       "3           1           0  ...                        72             89   \n",
       "4           0           0  ...                        53             53   \n",
       "\n",
       "   min_per_card_overall  min_per_assist_overall  cards_per_90_overall  \\\n",
       "0                  3420                       0                  0.03   \n",
       "1                     0                       0                  0.00   \n",
       "2                  2602                    2602                  0.03   \n",
       "3                   236                     648                  0.38   \n",
       "4                   319                       0                  0.28   \n",
       "\n",
       "   rank_in_league_top_attackers  rank_in_league_top_midfielders  \\\n",
       "0                           310                             419   \n",
       "1                           411                             366   \n",
       "2                           263                             249   \n",
       "3                           262                             104   \n",
       "4                           340                             271   \n",
       "\n",
       "   rank_in_league_top_defenders  rank_in_club_top_scorer  market_value  \n",
       "0                            90                       18      56000000  \n",
       "1                           102                       25       5500000  \n",
       "2                            88                       14      22000000  \n",
       "3                            79                       13      16500000  \n",
       "4                           142                       26      16500000  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lass_predictor.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lass, X_test_lass, y_train_lass, y_test_lass = train_test_split(lass_predictor, lass_target, random_state = 42, test_size = 0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lass = np.log(X_train_lass + 1)\n",
    "X_test_lass = np.log(X_test_lass + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lass_X_train_scaled = ss.fit_transform(X_train_lass)\n",
    "lass_X_test_scaled = ss.transform(X_test_lass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lass_X_train_all = pd.concat([pd.DataFrame(lass_X_train_scaled), cat_train_df], axis = 1)\n",
    "lass_X_test_all = pd.concat([pd.DataFrame(lass_X_test_scaled), cat_test_df], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso\n",
      "Training r^2: 0.42076149608872654\n",
      "Testing r^2: 0.43668899386434934\n",
      "Training MSE: 851132376.236633\n",
      "Testing MSE: 804887043.7751181\n",
      "\n",
      "\n",
      "Ridge\n",
      "Training r^2: 0.4207957386004175\n",
      "Testing r^2: 0.4364615610167731\n",
      "Training MSE: 851082060.3302368\n",
      "Testing MSE: 805212011.2448622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yusufolodo/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 952026121668.4601, tolerance: 344574024.13646054\n",
      "  positive)\n"
     ]
    }
   ],
   "source": [
    "lasso = Lasso(alpha=0.2) #Lasso is also known as the L1 norm.\n",
    "lasso.fit(lass_X_train_all, y_train_lass)\n",
    "print( 'Lasso')\n",
    "print('Training r^2:', lasso.score(lass_X_train_all, y_train_lass))\n",
    "print('Testing r^2:', lasso.score(lass_X_test_all, y_test_lass))\n",
    "print('Training MSE:', mean_squared_error(y_train_lass, lasso.predict(lass_X_train_all)))\n",
    "print('Testing MSE:', mean_squared_error(y_test_lass, lasso.predict(lass_X_test_all)))\n",
    "\n",
    "print(\"\\n\")\n",
    "    \n",
    "ridge = Ridge(alpha = 0.2) #Ridge is also known as the L2 norm.\n",
    "ridge.fit(lass_X_train_all, y_train_lass)\n",
    "print('Ridge')\n",
    "print('Training r^2:', ridge.score(lass_X_train_all, y_train_lass))\n",
    "print('Testing r^2:', ridge.score(lass_X_test_all, y_test_lass))\n",
    "print('Training MSE:', mean_squared_error(y_train_lass, ridge.predict(lass_X_train_all)))\n",
    "print('Testing MSE:', mean_squared_error(y_test_lass, ridge.predict(lass_X_test_all)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Ridge and Lasso doing both feature selection and paramter coefficient penalizing, the model performs worse than the earlier versions of it but these models also display the attribute of not overfitting and genearlizing to the test data. As more features were added, the Ridge and Lasso severely penalised the models and the MSE is also greater"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### USING SCI-KIT LEARN RECURSIVE FEATURE ELIMINATION METHOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model,X_train,X_test,y_train,y_test):\n",
    "    \n",
    "    print('Training R^2 :',model.score(X_train,y_train))\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    print('Training Root Mean Square Error',np.sqrt(metrics.mean_squared_error(y_train,y_pred_train)))\n",
    "    print('\\n----------------\\n')\n",
    "    print('Testing R^2 :',model.score(X_test,y_test))\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    print('Testing Root Mean Square Error',np.sqrt(metrics.mean_squared_error(y_test,y_pred_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training R^2 : 0.3832199693374999\n",
      "Training Root Mean Square Error 30104.747989003252\n",
      "\n",
      "----------------\n",
      "\n",
      "Testing R^2 : 0.39045865164235266\n",
      "Testing Root Mean Square Error 29511.74812688597\n",
      "The optimal number of features is:  31\n"
     ]
    }
   ],
   "source": [
    "rfe = RFECV(LinearRegression(),cv=5)\n",
    "X_rfe_train = rfe.fit_transform(X_train_lass,y_train_lass)\n",
    "X_rfe_test = rfe.transform(X_test_lass)\n",
    "lm = LinearRegression().fit(X_rfe_train,y_train)\n",
    "run_model(lm,X_rfe_train,X_rfe_test,y_train_lass,y_test_lass)\n",
    "print (\"The optimal number of features is: \",rfe.n_features_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSE massively reduces compared to the other regularisation models, the optimal number of features is 31 but the R-squared value is the worst amongst all built models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENSEMBLE METHOD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The r-squared for DTR is : 0.30243926425606493\n",
      "The mean_squared_error for DTR is : 996709796.7749062\n"
     ]
    }
   ],
   "source": [
    "#using a decision tree regressor\n",
    "regressor = DecisionTreeRegressor(random_state = 42)\n",
    "regressor.fit(X_train_lass, y_train_lass)\n",
    "print('The r-squared for DTR is :', r2_score(y_test_lass, regressor.predict(X_test_lass)))\n",
    "print('The mean_squared_error for DTR is :', mean_squared_error(y_test_lass, regressor.predict(X_test_lass)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The r-squared for DTR is : 0.38745462156531796\n",
      "The mean_squared_error for DTR is : 875235586.4811133\n"
     ]
    }
   ],
   "source": [
    "regressor = DecisionTreeRegressor(random_state = 42)\n",
    "regressor.fit(X_train_all, y_train)\n",
    "print('The r-squared for DTR is :', r2_score(y_test, regressor.predict(X_test_all)))\n",
    "print('The mean_squared_error for DTR is :', mean_squared_error(y_test_lass, regressor.predict(X_test_all)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using XGBOOST ensemble learner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:13:49] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yusufolodo/anaconda3/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score: 0.83\n",
      "mean_squared_error for train: 2.5e+08\n",
      "Validation score: 0.67\n",
      "mean_squared_error for test: 4.7e+08\n"
     ]
    }
   ],
   "source": [
    "xg_regressor = XGBRegressor(random_state=42)\n",
    "xg_regressor.fit(X_train_lass, y_train_lass)\n",
    "\n",
    "training_preds = xg_regressor.predict(X_train_lass)\n",
    "test_preds = xg_regressor.predict(X_test_lass)\n",
    "\n",
    "training_score = r2_score(y_train_lass, training_preds)\n",
    "mean_squared_error_train = mean_squared_error (y_train_lass, training_preds)\n",
    "test_score = r2_score(y_test_lass, test_preds)\n",
    "mean_squared_error_test = mean_squared_error (y_test_lass, test_preds)\n",
    "\n",
    "print('Training score: {:.2}'.format(training_score ))\n",
    "print('mean_squared_error for train: {:.2}'.format(mean_squared_error_train))\n",
    "print('Validation score: {:.2}'.format(test_score ))\n",
    "print('mean_squared_error for test: {:.2}'.format(mean_squared_error_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:13:50] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yusufolodo/anaconda3/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score: 0.83\n",
      "mean_squared_error for train: 2.4e+08\n",
      "Validation score: 0.73\n",
      "mean_squared_error for test: 3.9e+08\n"
     ]
    }
   ],
   "source": [
    "xg_regressor = XGBRegressor(random_state=42)\n",
    "xg_regressor.fit(X_train_all, y_train)\n",
    "\n",
    "training_preds = xg_regressor.predict(X_train_all)\n",
    "test_preds = xg_regressor.predict(X_test_all)\n",
    "\n",
    "training_score = r2_score(y_train, training_preds)\n",
    "mean_squared_error_train = mean_squared_error (y_train, training_preds)\n",
    "test_score = r2_score(y_test, test_preds)\n",
    "mean_squared_error_test = mean_squared_error (y_test, test_preds)\n",
    "\n",
    "print('Training score: {:.2}'.format(training_score ))\n",
    "print('mean_squared_error for train: {:.2}'.format(mean_squared_error_train))\n",
    "print('Validation score: {:.2}'.format(test_score ))\n",
    "print('mean_squared_error for test: {:.2}'.format(mean_squared_error_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the best training score yet on the dataset but the difference in the mean square error for the train and test data set means the model is overfitting to the training data and not genearlizing to new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb1 = XGBRegressor()\n",
    "parameters = {'nthread':[4], #when use hyperthread, xgboost may become slower\n",
    "              'objective':['reg:linear'],\n",
    "              'learning_rate': [0.01,.03, 0.05, .07], #so called `eta` value\n",
    "              'max_depth': [2,3,4,5, 6,],\n",
    "              'min_child_weight': [4],\n",
    "              'silent': [1],\n",
    "              'subsample': [0.15,0.25,0.7],\n",
    "              'colsample_bytree': [0.7],\n",
    "              'n_estimators': [500]}\n",
    "\n",
    "xgb_grid = GridSearchCV(xgb1,\n",
    "                        parameters,\n",
    "                        cv = 5,\n",
    "                        n_jobs = -1,\n",
    "                        verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yusufolodo/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   45.0s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  6.6min finished\n",
      "/Users/yusufolodo/anaconda3/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=XGBRegressor(base_score=0.5, booster='gbtree',\n",
       "                                    colsample_bylevel=1, colsample_bynode=1,\n",
       "                                    colsample_bytree=1, gamma=0,\n",
       "                                    importance_type='gain', learning_rate=0.1,\n",
       "                                    max_delta_step=0, max_depth=3,\n",
       "                                    min_child_weight=1, missing=None,\n",
       "                                    n_estimators=100, n_jobs=1, nthread=None,\n",
       "                                    objective='reg:linear', random_state=0,\n",
       "                                    reg_alpha=0, reg_...\n",
       "                                    subsample=1, verbosity=1),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'colsample_bytree': [0.7],\n",
       "                         'learning_rate': [0.01, 0.03, 0.05, 0.07],\n",
       "                         'max_depth': [2, 3, 4, 5, 6], 'min_child_weight': [4],\n",
       "                         'n_estimators': [500], 'nthread': [4],\n",
       "                         'objective': ['reg:linear'], 'silent': [1],\n",
       "                         'subsample': [0.15, 0.25, 0.7]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=True)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_grid.fit(X_train_all.as_matrix(), y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7009692865838489\n",
      "{'colsample_bytree': 0.7, 'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 4, 'n_estimators': 500, 'nthread': 4, 'objective': 'reg:linear', 'silent': 1, 'subsample': 0.25}\n"
     ]
    }
   ],
   "source": [
    "print(xgb_grid.best_score_)\n",
    "print(xgb_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score: 0.83\n",
      "mean_squared_error for train: 2.5e+08\n",
      "test score: 0.76\n",
      "mean_squared_error for test: 3.5e+08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yusufolodo/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/yusufolodo/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "xgb_train = xgb_grid.predict(X_train_all.as_matrix())\n",
    "xgb_predictions = xgb_grid.predict(X_test_all.as_matrix())\n",
    "\n",
    "\n",
    "training_score = r2_score(y_train, xgb_train)\n",
    "mean_squared_error_train = mean_squared_error (y_train, xgb_train)\n",
    "\n",
    "test_score = r2_score(y_test, xgb_predictions)\n",
    "mean_squared_error_test = mean_squared_error (y_test, xgb_predictions)\n",
    "\n",
    "print('Training score: {:.2}'.format(training_score ))\n",
    "print('mean_squared_error for train: {:.2}'.format(mean_squared_error_train))\n",
    "print('test score: {:.2}'.format(test_score ))\n",
    "print('mean_squared_error for test: {:.2}'.format(mean_squared_error_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has  r-squared of 0.83 for the train set and 0.76 for test set and the mean squared error improved compared to the non-hyperparamter tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = cosine_similarity(X_train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>...</th>\n",
       "      <th>f2335</th>\n",
       "      <th>f2336</th>\n",
       "      <th>f2337</th>\n",
       "      <th>f2338</th>\n",
       "      <th>f2339</th>\n",
       "      <th>f2340</th>\n",
       "      <th>f2341</th>\n",
       "      <th>f2342</th>\n",
       "      <th>f2343</th>\n",
       "      <th>f2344</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.091952</td>\n",
       "      <td>-0.228092</td>\n",
       "      <td>0.618863</td>\n",
       "      <td>0.095940</td>\n",
       "      <td>-0.452772</td>\n",
       "      <td>0.404534</td>\n",
       "      <td>-0.203345</td>\n",
       "      <td>-0.046381</td>\n",
       "      <td>0.096728</td>\n",
       "      <td>...</td>\n",
       "      <td>0.931204</td>\n",
       "      <td>0.811829</td>\n",
       "      <td>-0.028316</td>\n",
       "      <td>-0.347458</td>\n",
       "      <td>-0.111570</td>\n",
       "      <td>-0.217924</td>\n",
       "      <td>0.437466</td>\n",
       "      <td>-0.034023</td>\n",
       "      <td>0.719556</td>\n",
       "      <td>0.329240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.091952</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.536741</td>\n",
       "      <td>0.241940</td>\n",
       "      <td>0.295111</td>\n",
       "      <td>-0.055307</td>\n",
       "      <td>0.244672</td>\n",
       "      <td>-0.246842</td>\n",
       "      <td>0.233113</td>\n",
       "      <td>0.205742</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.131102</td>\n",
       "      <td>-0.058291</td>\n",
       "      <td>-0.050034</td>\n",
       "      <td>0.232764</td>\n",
       "      <td>-0.414572</td>\n",
       "      <td>0.288268</td>\n",
       "      <td>0.419675</td>\n",
       "      <td>-0.066852</td>\n",
       "      <td>0.115123</td>\n",
       "      <td>0.120528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.228092</td>\n",
       "      <td>0.536741</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.107603</td>\n",
       "      <td>0.595426</td>\n",
       "      <td>0.036437</td>\n",
       "      <td>-0.086371</td>\n",
       "      <td>-0.232557</td>\n",
       "      <td>-0.043840</td>\n",
       "      <td>0.142447</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.244392</td>\n",
       "      <td>-0.325503</td>\n",
       "      <td>0.208724</td>\n",
       "      <td>0.162139</td>\n",
       "      <td>-0.465973</td>\n",
       "      <td>0.783573</td>\n",
       "      <td>0.402496</td>\n",
       "      <td>-0.495655</td>\n",
       "      <td>0.028516</td>\n",
       "      <td>0.349114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.618863</td>\n",
       "      <td>0.241940</td>\n",
       "      <td>-0.107603</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.114722</td>\n",
       "      <td>-0.382983</td>\n",
       "      <td>0.621728</td>\n",
       "      <td>-0.426925</td>\n",
       "      <td>-0.101264</td>\n",
       "      <td>0.156987</td>\n",
       "      <td>...</td>\n",
       "      <td>0.572217</td>\n",
       "      <td>0.700180</td>\n",
       "      <td>-0.351481</td>\n",
       "      <td>-0.242431</td>\n",
       "      <td>-0.331505</td>\n",
       "      <td>-0.245441</td>\n",
       "      <td>0.640243</td>\n",
       "      <td>-0.098366</td>\n",
       "      <td>0.584549</td>\n",
       "      <td>0.308257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.095940</td>\n",
       "      <td>0.295111</td>\n",
       "      <td>0.595426</td>\n",
       "      <td>0.114722</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.130959</td>\n",
       "      <td>-0.070642</td>\n",
       "      <td>-0.183933</td>\n",
       "      <td>-0.155248</td>\n",
       "      <td>0.333976</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074299</td>\n",
       "      <td>0.177520</td>\n",
       "      <td>-0.005235</td>\n",
       "      <td>-0.028086</td>\n",
       "      <td>-0.546721</td>\n",
       "      <td>0.678323</td>\n",
       "      <td>0.512035</td>\n",
       "      <td>-0.577655</td>\n",
       "      <td>0.334680</td>\n",
       "      <td>0.718015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.452772</td>\n",
       "      <td>-0.055307</td>\n",
       "      <td>0.036437</td>\n",
       "      <td>-0.382983</td>\n",
       "      <td>-0.130959</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.434480</td>\n",
       "      <td>0.484009</td>\n",
       "      <td>0.147593</td>\n",
       "      <td>0.173007</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.219138</td>\n",
       "      <td>-0.443589</td>\n",
       "      <td>0.009969</td>\n",
       "      <td>0.482159</td>\n",
       "      <td>0.545274</td>\n",
       "      <td>-0.032354</td>\n",
       "      <td>-0.476168</td>\n",
       "      <td>0.161888</td>\n",
       "      <td>-0.504275</td>\n",
       "      <td>-0.119813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.404534</td>\n",
       "      <td>0.244672</td>\n",
       "      <td>-0.086371</td>\n",
       "      <td>0.621728</td>\n",
       "      <td>-0.070642</td>\n",
       "      <td>-0.434480</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.289321</td>\n",
       "      <td>-0.051074</td>\n",
       "      <td>0.090265</td>\n",
       "      <td>...</td>\n",
       "      <td>0.253142</td>\n",
       "      <td>0.429103</td>\n",
       "      <td>0.014655</td>\n",
       "      <td>-0.345535</td>\n",
       "      <td>-0.167917</td>\n",
       "      <td>-0.318172</td>\n",
       "      <td>0.414641</td>\n",
       "      <td>0.307558</td>\n",
       "      <td>0.295356</td>\n",
       "      <td>0.014585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.203345</td>\n",
       "      <td>-0.246842</td>\n",
       "      <td>-0.232557</td>\n",
       "      <td>-0.426925</td>\n",
       "      <td>-0.183933</td>\n",
       "      <td>0.484009</td>\n",
       "      <td>-0.289321</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.145294</td>\n",
       "      <td>-0.379738</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.056494</td>\n",
       "      <td>-0.154971</td>\n",
       "      <td>0.078791</td>\n",
       "      <td>0.450788</td>\n",
       "      <td>0.813208</td>\n",
       "      <td>-0.070878</td>\n",
       "      <td>-0.436717</td>\n",
       "      <td>0.303382</td>\n",
       "      <td>-0.290524</td>\n",
       "      <td>-0.251623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.046381</td>\n",
       "      <td>0.233113</td>\n",
       "      <td>-0.043840</td>\n",
       "      <td>-0.101264</td>\n",
       "      <td>-0.155248</td>\n",
       "      <td>0.147593</td>\n",
       "      <td>-0.051074</td>\n",
       "      <td>0.145294</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.183684</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.101818</td>\n",
       "      <td>0.036631</td>\n",
       "      <td>-0.090484</td>\n",
       "      <td>0.635226</td>\n",
       "      <td>-0.005188</td>\n",
       "      <td>-0.088938</td>\n",
       "      <td>-0.015307</td>\n",
       "      <td>0.621802</td>\n",
       "      <td>0.298274</td>\n",
       "      <td>-0.324754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.096728</td>\n",
       "      <td>0.205742</td>\n",
       "      <td>0.142447</td>\n",
       "      <td>0.156987</td>\n",
       "      <td>0.333976</td>\n",
       "      <td>0.173007</td>\n",
       "      <td>0.090265</td>\n",
       "      <td>-0.379738</td>\n",
       "      <td>0.183684</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.065208</td>\n",
       "      <td>0.163637</td>\n",
       "      <td>-0.070326</td>\n",
       "      <td>-0.034603</td>\n",
       "      <td>-0.208769</td>\n",
       "      <td>-0.049495</td>\n",
       "      <td>0.112326</td>\n",
       "      <td>0.046470</td>\n",
       "      <td>0.142126</td>\n",
       "      <td>0.532926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.205173</td>\n",
       "      <td>-0.362432</td>\n",
       "      <td>-0.506833</td>\n",
       "      <td>0.082844</td>\n",
       "      <td>-0.288328</td>\n",
       "      <td>-0.018812</td>\n",
       "      <td>0.302395</td>\n",
       "      <td>0.299759</td>\n",
       "      <td>-0.148872</td>\n",
       "      <td>-0.086679</td>\n",
       "      <td>...</td>\n",
       "      <td>0.141940</td>\n",
       "      <td>0.331129</td>\n",
       "      <td>0.396786</td>\n",
       "      <td>-0.347832</td>\n",
       "      <td>0.439777</td>\n",
       "      <td>-0.488280</td>\n",
       "      <td>-0.246742</td>\n",
       "      <td>0.321045</td>\n",
       "      <td>-0.110490</td>\n",
       "      <td>-0.284319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.094855</td>\n",
       "      <td>-0.080101</td>\n",
       "      <td>-0.071581</td>\n",
       "      <td>0.128168</td>\n",
       "      <td>0.106704</td>\n",
       "      <td>-0.216091</td>\n",
       "      <td>-0.014079</td>\n",
       "      <td>-0.169692</td>\n",
       "      <td>-0.137535</td>\n",
       "      <td>-0.042936</td>\n",
       "      <td>...</td>\n",
       "      <td>0.096248</td>\n",
       "      <td>0.217477</td>\n",
       "      <td>-0.115612</td>\n",
       "      <td>-0.204029</td>\n",
       "      <td>-0.207704</td>\n",
       "      <td>-0.004520</td>\n",
       "      <td>0.095213</td>\n",
       "      <td>-0.161372</td>\n",
       "      <td>0.107352</td>\n",
       "      <td>0.051541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.630444</td>\n",
       "      <td>0.480847</td>\n",
       "      <td>-0.021312</td>\n",
       "      <td>0.836366</td>\n",
       "      <td>0.199441</td>\n",
       "      <td>-0.521451</td>\n",
       "      <td>0.601003</td>\n",
       "      <td>-0.456145</td>\n",
       "      <td>-0.162569</td>\n",
       "      <td>0.155967</td>\n",
       "      <td>...</td>\n",
       "      <td>0.586304</td>\n",
       "      <td>0.690053</td>\n",
       "      <td>-0.326261</td>\n",
       "      <td>-0.437084</td>\n",
       "      <td>-0.376514</td>\n",
       "      <td>-0.142936</td>\n",
       "      <td>0.698955</td>\n",
       "      <td>-0.203424</td>\n",
       "      <td>0.612648</td>\n",
       "      <td>0.387185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.074358</td>\n",
       "      <td>-0.196867</td>\n",
       "      <td>0.002707</td>\n",
       "      <td>-0.385698</td>\n",
       "      <td>-0.198083</td>\n",
       "      <td>0.043938</td>\n",
       "      <td>0.055254</td>\n",
       "      <td>0.358756</td>\n",
       "      <td>0.214126</td>\n",
       "      <td>-0.177768</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.238928</td>\n",
       "      <td>-0.243237</td>\n",
       "      <td>0.625709</td>\n",
       "      <td>0.074065</td>\n",
       "      <td>0.369167</td>\n",
       "      <td>0.056110</td>\n",
       "      <td>-0.181792</td>\n",
       "      <td>0.275437</td>\n",
       "      <td>-0.060148</td>\n",
       "      <td>-0.303494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.157613</td>\n",
       "      <td>-0.267568</td>\n",
       "      <td>-0.113418</td>\n",
       "      <td>-0.276857</td>\n",
       "      <td>-0.223162</td>\n",
       "      <td>0.134381</td>\n",
       "      <td>-0.221998</td>\n",
       "      <td>0.111499</td>\n",
       "      <td>-0.208795</td>\n",
       "      <td>-0.218465</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.103852</td>\n",
       "      <td>-0.232842</td>\n",
       "      <td>0.116779</td>\n",
       "      <td>-0.114919</td>\n",
       "      <td>0.208413</td>\n",
       "      <td>-0.050737</td>\n",
       "      <td>-0.275773</td>\n",
       "      <td>-0.073703</td>\n",
       "      <td>-0.299126</td>\n",
       "      <td>-0.192836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.115701</td>\n",
       "      <td>0.315200</td>\n",
       "      <td>0.348498</td>\n",
       "      <td>-0.197933</td>\n",
       "      <td>-0.093127</td>\n",
       "      <td>0.140925</td>\n",
       "      <td>0.229698</td>\n",
       "      <td>0.013077</td>\n",
       "      <td>0.126761</td>\n",
       "      <td>0.075164</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.275134</td>\n",
       "      <td>-0.390172</td>\n",
       "      <td>0.722076</td>\n",
       "      <td>0.110851</td>\n",
       "      <td>0.022663</td>\n",
       "      <td>0.054527</td>\n",
       "      <td>-0.100355</td>\n",
       "      <td>0.207660</td>\n",
       "      <td>-0.349288</td>\n",
       "      <td>-0.309254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.056840</td>\n",
       "      <td>0.500090</td>\n",
       "      <td>0.747729</td>\n",
       "      <td>0.214326</td>\n",
       "      <td>0.833817</td>\n",
       "      <td>-0.250546</td>\n",
       "      <td>0.079024</td>\n",
       "      <td>-0.131620</td>\n",
       "      <td>-0.187582</td>\n",
       "      <td>0.030426</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047779</td>\n",
       "      <td>0.102000</td>\n",
       "      <td>0.005821</td>\n",
       "      <td>0.008877</td>\n",
       "      <td>-0.514423</td>\n",
       "      <td>0.755235</td>\n",
       "      <td>0.673480</td>\n",
       "      <td>-0.626833</td>\n",
       "      <td>0.314077</td>\n",
       "      <td>0.562174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.293461</td>\n",
       "      <td>0.338556</td>\n",
       "      <td>0.025891</td>\n",
       "      <td>-0.050765</td>\n",
       "      <td>-0.340369</td>\n",
       "      <td>0.484741</td>\n",
       "      <td>0.102588</td>\n",
       "      <td>0.110214</td>\n",
       "      <td>0.426685</td>\n",
       "      <td>0.473560</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.227881</td>\n",
       "      <td>-0.187121</td>\n",
       "      <td>-0.047974</td>\n",
       "      <td>0.328979</td>\n",
       "      <td>0.288618</td>\n",
       "      <td>-0.332002</td>\n",
       "      <td>-0.224720</td>\n",
       "      <td>0.477264</td>\n",
       "      <td>-0.323142</td>\n",
       "      <td>-0.216733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.786851</td>\n",
       "      <td>0.248639</td>\n",
       "      <td>-0.234083</td>\n",
       "      <td>0.684762</td>\n",
       "      <td>0.115605</td>\n",
       "      <td>-0.508270</td>\n",
       "      <td>0.412974</td>\n",
       "      <td>-0.301636</td>\n",
       "      <td>0.101072</td>\n",
       "      <td>0.164206</td>\n",
       "      <td>...</td>\n",
       "      <td>0.720093</td>\n",
       "      <td>0.840504</td>\n",
       "      <td>-0.346336</td>\n",
       "      <td>-0.239594</td>\n",
       "      <td>-0.260939</td>\n",
       "      <td>-0.246213</td>\n",
       "      <td>0.468512</td>\n",
       "      <td>0.044211</td>\n",
       "      <td>0.764051</td>\n",
       "      <td>0.302741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.094330</td>\n",
       "      <td>0.219825</td>\n",
       "      <td>-0.024773</td>\n",
       "      <td>0.026329</td>\n",
       "      <td>-0.031680</td>\n",
       "      <td>-0.114497</td>\n",
       "      <td>-0.048558</td>\n",
       "      <td>-0.121913</td>\n",
       "      <td>0.231383</td>\n",
       "      <td>0.042007</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059182</td>\n",
       "      <td>0.135624</td>\n",
       "      <td>-0.170091</td>\n",
       "      <td>0.122802</td>\n",
       "      <td>-0.165309</td>\n",
       "      <td>-0.066973</td>\n",
       "      <td>-0.016566</td>\n",
       "      <td>0.126481</td>\n",
       "      <td>0.158024</td>\n",
       "      <td>-0.083481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.453936</td>\n",
       "      <td>0.177905</td>\n",
       "      <td>0.403509</td>\n",
       "      <td>-0.047095</td>\n",
       "      <td>0.336509</td>\n",
       "      <td>0.184544</td>\n",
       "      <td>-0.197102</td>\n",
       "      <td>0.107504</td>\n",
       "      <td>0.216431</td>\n",
       "      <td>0.137269</td>\n",
       "      <td>...</td>\n",
       "      <td>0.519120</td>\n",
       "      <td>0.140863</td>\n",
       "      <td>0.198241</td>\n",
       "      <td>0.288270</td>\n",
       "      <td>-0.042915</td>\n",
       "      <td>0.397326</td>\n",
       "      <td>0.198305</td>\n",
       "      <td>-0.154657</td>\n",
       "      <td>0.340225</td>\n",
       "      <td>0.308956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.056175</td>\n",
       "      <td>-0.316117</td>\n",
       "      <td>-0.338475</td>\n",
       "      <td>-0.250340</td>\n",
       "      <td>-0.427288</td>\n",
       "      <td>0.090305</td>\n",
       "      <td>0.117197</td>\n",
       "      <td>0.388927</td>\n",
       "      <td>-0.080885</td>\n",
       "      <td>-0.238096</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046021</td>\n",
       "      <td>-0.198983</td>\n",
       "      <td>0.786570</td>\n",
       "      <td>-0.071625</td>\n",
       "      <td>0.522149</td>\n",
       "      <td>-0.250719</td>\n",
       "      <td>-0.409929</td>\n",
       "      <td>0.344591</td>\n",
       "      <td>-0.353215</td>\n",
       "      <td>-0.396099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.404759</td>\n",
       "      <td>-0.178727</td>\n",
       "      <td>0.004029</td>\n",
       "      <td>-0.601058</td>\n",
       "      <td>-0.314270</td>\n",
       "      <td>0.651047</td>\n",
       "      <td>-0.409019</td>\n",
       "      <td>0.860074</td>\n",
       "      <td>0.192836</td>\n",
       "      <td>-0.311147</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.243836</td>\n",
       "      <td>-0.479054</td>\n",
       "      <td>0.158774</td>\n",
       "      <td>0.537446</td>\n",
       "      <td>0.783195</td>\n",
       "      <td>0.016409</td>\n",
       "      <td>-0.540115</td>\n",
       "      <td>0.297405</td>\n",
       "      <td>-0.460076</td>\n",
       "      <td>-0.320710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.808443</td>\n",
       "      <td>-0.005603</td>\n",
       "      <td>-0.252456</td>\n",
       "      <td>0.709752</td>\n",
       "      <td>0.107622</td>\n",
       "      <td>-0.522757</td>\n",
       "      <td>0.436376</td>\n",
       "      <td>-0.330299</td>\n",
       "      <td>0.225524</td>\n",
       "      <td>0.166853</td>\n",
       "      <td>...</td>\n",
       "      <td>0.742065</td>\n",
       "      <td>0.861089</td>\n",
       "      <td>-0.342571</td>\n",
       "      <td>-0.307091</td>\n",
       "      <td>-0.260567</td>\n",
       "      <td>-0.200967</td>\n",
       "      <td>0.636068</td>\n",
       "      <td>0.017062</td>\n",
       "      <td>0.928011</td>\n",
       "      <td>0.314612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.220514</td>\n",
       "      <td>0.645305</td>\n",
       "      <td>0.684871</td>\n",
       "      <td>0.111217</td>\n",
       "      <td>0.180463</td>\n",
       "      <td>0.106894</td>\n",
       "      <td>0.168093</td>\n",
       "      <td>-0.135382</td>\n",
       "      <td>0.317014</td>\n",
       "      <td>0.201675</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.234784</td>\n",
       "      <td>-0.175811</td>\n",
       "      <td>-0.023810</td>\n",
       "      <td>0.340178</td>\n",
       "      <td>-0.288345</td>\n",
       "      <td>0.220424</td>\n",
       "      <td>0.258329</td>\n",
       "      <td>0.055706</td>\n",
       "      <td>-0.031163</td>\n",
       "      <td>-0.013899</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25 rows × 2345 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          f0        f1        f2        f3        f4        f5        f6  \\\n",
       "0   1.000000 -0.091952 -0.228092  0.618863  0.095940 -0.452772  0.404534   \n",
       "1  -0.091952  1.000000  0.536741  0.241940  0.295111 -0.055307  0.244672   \n",
       "2  -0.228092  0.536741  1.000000 -0.107603  0.595426  0.036437 -0.086371   \n",
       "3   0.618863  0.241940 -0.107603  1.000000  0.114722 -0.382983  0.621728   \n",
       "4   0.095940  0.295111  0.595426  0.114722  1.000000 -0.130959 -0.070642   \n",
       "5  -0.452772 -0.055307  0.036437 -0.382983 -0.130959  1.000000 -0.434480   \n",
       "6   0.404534  0.244672 -0.086371  0.621728 -0.070642 -0.434480  1.000000   \n",
       "7  -0.203345 -0.246842 -0.232557 -0.426925 -0.183933  0.484009 -0.289321   \n",
       "8  -0.046381  0.233113 -0.043840 -0.101264 -0.155248  0.147593 -0.051074   \n",
       "9   0.096728  0.205742  0.142447  0.156987  0.333976  0.173007  0.090265   \n",
       "10  0.205173 -0.362432 -0.506833  0.082844 -0.288328 -0.018812  0.302395   \n",
       "11  0.094855 -0.080101 -0.071581  0.128168  0.106704 -0.216091 -0.014079   \n",
       "12  0.630444  0.480847 -0.021312  0.836366  0.199441 -0.521451  0.601003   \n",
       "13 -0.074358 -0.196867  0.002707 -0.385698 -0.198083  0.043938  0.055254   \n",
       "14 -0.157613 -0.267568 -0.113418 -0.276857 -0.223162  0.134381 -0.221998   \n",
       "15 -0.115701  0.315200  0.348498 -0.197933 -0.093127  0.140925  0.229698   \n",
       "16  0.056840  0.500090  0.747729  0.214326  0.833817 -0.250546  0.079024   \n",
       "17 -0.293461  0.338556  0.025891 -0.050765 -0.340369  0.484741  0.102588   \n",
       "18  0.786851  0.248639 -0.234083  0.684762  0.115605 -0.508270  0.412974   \n",
       "19  0.094330  0.219825 -0.024773  0.026329 -0.031680 -0.114497 -0.048558   \n",
       "20  0.453936  0.177905  0.403509 -0.047095  0.336509  0.184544 -0.197102   \n",
       "21  0.056175 -0.316117 -0.338475 -0.250340 -0.427288  0.090305  0.117197   \n",
       "22 -0.404759 -0.178727  0.004029 -0.601058 -0.314270  0.651047 -0.409019   \n",
       "23  0.808443 -0.005603 -0.252456  0.709752  0.107622 -0.522757  0.436376   \n",
       "24 -0.220514  0.645305  0.684871  0.111217  0.180463  0.106894  0.168093   \n",
       "\n",
       "          f7        f8        f9  ...     f2335     f2336     f2337     f2338  \\\n",
       "0  -0.203345 -0.046381  0.096728  ...  0.931204  0.811829 -0.028316 -0.347458   \n",
       "1  -0.246842  0.233113  0.205742  ... -0.131102 -0.058291 -0.050034  0.232764   \n",
       "2  -0.232557 -0.043840  0.142447  ... -0.244392 -0.325503  0.208724  0.162139   \n",
       "3  -0.426925 -0.101264  0.156987  ...  0.572217  0.700180 -0.351481 -0.242431   \n",
       "4  -0.183933 -0.155248  0.333976  ...  0.074299  0.177520 -0.005235 -0.028086   \n",
       "5   0.484009  0.147593  0.173007  ... -0.219138 -0.443589  0.009969  0.482159   \n",
       "6  -0.289321 -0.051074  0.090265  ...  0.253142  0.429103  0.014655 -0.345535   \n",
       "7   1.000000  0.145294 -0.379738  ... -0.056494 -0.154971  0.078791  0.450788   \n",
       "8   0.145294  1.000000  0.183684  ... -0.101818  0.036631 -0.090484  0.635226   \n",
       "9  -0.379738  0.183684  1.000000  ...  0.065208  0.163637 -0.070326 -0.034603   \n",
       "10  0.299759 -0.148872 -0.086679  ...  0.141940  0.331129  0.396786 -0.347832   \n",
       "11 -0.169692 -0.137535 -0.042936  ...  0.096248  0.217477 -0.115612 -0.204029   \n",
       "12 -0.456145 -0.162569  0.155967  ...  0.586304  0.690053 -0.326261 -0.437084   \n",
       "13  0.358756  0.214126 -0.177768  ... -0.238928 -0.243237  0.625709  0.074065   \n",
       "14  0.111499 -0.208795 -0.218465  ... -0.103852 -0.232842  0.116779 -0.114919   \n",
       "15  0.013077  0.126761  0.075164  ... -0.275134 -0.390172  0.722076  0.110851   \n",
       "16 -0.131620 -0.187582  0.030426  ...  0.047779  0.102000  0.005821  0.008877   \n",
       "17  0.110214  0.426685  0.473560  ... -0.227881 -0.187121 -0.047974  0.328979   \n",
       "18 -0.301636  0.101072  0.164206  ...  0.720093  0.840504 -0.346336 -0.239594   \n",
       "19 -0.121913  0.231383  0.042007  ...  0.059182  0.135624 -0.170091  0.122802   \n",
       "20  0.107504  0.216431  0.137269  ...  0.519120  0.140863  0.198241  0.288270   \n",
       "21  0.388927 -0.080885 -0.238096  ... -0.046021 -0.198983  0.786570 -0.071625   \n",
       "22  0.860074  0.192836 -0.311147  ... -0.243836 -0.479054  0.158774  0.537446   \n",
       "23 -0.330299  0.225524  0.166853  ...  0.742065  0.861089 -0.342571 -0.307091   \n",
       "24 -0.135382  0.317014  0.201675  ... -0.234784 -0.175811 -0.023810  0.340178   \n",
       "\n",
       "       f2339     f2340     f2341     f2342     f2343     f2344  \n",
       "0  -0.111570 -0.217924  0.437466 -0.034023  0.719556  0.329240  \n",
       "1  -0.414572  0.288268  0.419675 -0.066852  0.115123  0.120528  \n",
       "2  -0.465973  0.783573  0.402496 -0.495655  0.028516  0.349114  \n",
       "3  -0.331505 -0.245441  0.640243 -0.098366  0.584549  0.308257  \n",
       "4  -0.546721  0.678323  0.512035 -0.577655  0.334680  0.718015  \n",
       "5   0.545274 -0.032354 -0.476168  0.161888 -0.504275 -0.119813  \n",
       "6  -0.167917 -0.318172  0.414641  0.307558  0.295356  0.014585  \n",
       "7   0.813208 -0.070878 -0.436717  0.303382 -0.290524 -0.251623  \n",
       "8  -0.005188 -0.088938 -0.015307  0.621802  0.298274 -0.324754  \n",
       "9  -0.208769 -0.049495  0.112326  0.046470  0.142126  0.532926  \n",
       "10  0.439777 -0.488280 -0.246742  0.321045 -0.110490 -0.284319  \n",
       "11 -0.207704 -0.004520  0.095213 -0.161372  0.107352  0.051541  \n",
       "12 -0.376514 -0.142936  0.698955 -0.203424  0.612648  0.387185  \n",
       "13  0.369167  0.056110 -0.181792  0.275437 -0.060148 -0.303494  \n",
       "14  0.208413 -0.050737 -0.275773 -0.073703 -0.299126 -0.192836  \n",
       "15  0.022663  0.054527 -0.100355  0.207660 -0.349288 -0.309254  \n",
       "16 -0.514423  0.755235  0.673480 -0.626833  0.314077  0.562174  \n",
       "17  0.288618 -0.332002 -0.224720  0.477264 -0.323142 -0.216733  \n",
       "18 -0.260939 -0.246213  0.468512  0.044211  0.764051  0.302741  \n",
       "19 -0.165309 -0.066973 -0.016566  0.126481  0.158024 -0.083481  \n",
       "20 -0.042915  0.397326  0.198305 -0.154657  0.340225  0.308956  \n",
       "21  0.522149 -0.250719 -0.409929  0.344591 -0.353215 -0.396099  \n",
       "22  0.783195  0.016409 -0.540115  0.297405 -0.460076 -0.320710  \n",
       "23 -0.260567 -0.200967  0.636068  0.017062  0.928011  0.314612  \n",
       "24 -0.288345  0.220424  0.258329  0.055706 -0.031163 -0.013899  \n",
       "\n",
       "[25 rows x 2345 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data = data[0:, 0:], \n",
    "                 index = [i for i in range(data.shape[0])],\n",
    "                  columns = ['f' + str(i) for i in range(data.shape[1])])\n",
    "df.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>assists_overall</th>\n",
       "      <th>penalty_goals</th>\n",
       "      <th>penalty_misses</th>\n",
       "      <th>minutes_played_overall</th>\n",
       "      <th>market_value</th>\n",
       "      <th>clean_sheets_overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2238</th>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>245</td>\n",
       "      <td>5000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2930</th>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1440</td>\n",
       "      <td>775000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2723</th>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1720</td>\n",
       "      <td>475000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>91</td>\n",
       "      <td>1100000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>553</td>\n",
       "      <td>500000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1483</th>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3060</td>\n",
       "      <td>27000000</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1263</th>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1003</td>\n",
       "      <td>1200000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1058</td>\n",
       "      <td>23000000</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017</td>\n",
       "      <td>700000</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3249</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1890</td>\n",
       "      <td>3000000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1599</td>\n",
       "      <td>11500000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>687</td>\n",
       "      <td>3100000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2952</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1200000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>28</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2233</td>\n",
       "      <td>6500000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2028</th>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2404</td>\n",
       "      <td>27000000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2166</th>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2594</td>\n",
       "      <td>2800000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>230</td>\n",
       "      <td>475000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3098</th>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2777</td>\n",
       "      <td>9000000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2984</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>124</td>\n",
       "      <td>725000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2934</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1520</td>\n",
       "      <td>950000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024</th>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1360</td>\n",
       "      <td>9000000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2044</th>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1730</td>\n",
       "      <td>10500000</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2687</th>\n",
       "      <td>31</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1971</td>\n",
       "      <td>27500000</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1114</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>675000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2601</th>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1724</td>\n",
       "      <td>2100000</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      age  assists_overall  penalty_goals  penalty_misses  \\\n",
       "2238   21                1              0               0   \n",
       "2930   28                0              0               0   \n",
       "2723   37                0              0               0   \n",
       "1798   20                0              0               0   \n",
       "123    32                0              0               0   \n",
       "1483   29                0              0               0   \n",
       "1263   22                2              0               0   \n",
       "220    28                3              0               0   \n",
       "819    25                0              0               0   \n",
       "3249   25                0              0               0   \n",
       "358    20                4              0               0   \n",
       "100    24                0              1               1   \n",
       "2952   21                0              0               0   \n",
       "551    28                6              0               0   \n",
       "2028   28                4              5               1   \n",
       "2166   28                3              0               0   \n",
       "259    34                0              0               0   \n",
       "3098   24                0              0               0   \n",
       "2984   20                0              0               0   \n",
       "2934   25                0              2               0   \n",
       "2024   29                0              0               0   \n",
       "2044   24                5              0               0   \n",
       "2687   31                3              0               0   \n",
       "1114   19                0              0               0   \n",
       "2601   28                0              0               0   \n",
       "\n",
       "      minutes_played_overall  market_value  clean_sheets_overall  \n",
       "2238                     245       5000000                     0  \n",
       "2930                    1440        775000                     5  \n",
       "2723                    1720        475000                     1  \n",
       "1798                      91       1100000                     1  \n",
       "123                      553        500000                     1  \n",
       "1483                    3060      27000000                     9  \n",
       "1263                    1003       1200000                     3  \n",
       "220                     1058      23000000                    12  \n",
       "819                     2017        700000                     9  \n",
       "3249                    1890       3000000                     3  \n",
       "358                     1599      11500000                     4  \n",
       "100                      687       3100000                     2  \n",
       "2952                       0       1200000                     0  \n",
       "551                     2233       6500000                     5  \n",
       "2028                    2404      27000000                     5  \n",
       "2166                    2594       2800000                     5  \n",
       "259                      230        475000                     1  \n",
       "3098                    2777       9000000                     8  \n",
       "2984                     124        725000                     1  \n",
       "2934                    1520        950000                     5  \n",
       "2024                    1360       9000000                     3  \n",
       "2044                    1730      10500000                     7  \n",
       "2687                    1971      27500000                    14  \n",
       "1114                       0        675000                     0  \n",
       "2601                    1724       2100000                     6  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_name</th>\n",
       "      <th>age</th>\n",
       "      <th>league</th>\n",
       "      <th>season</th>\n",
       "      <th>position</th>\n",
       "      <th>current_club</th>\n",
       "      <th>minutes_played_overall</th>\n",
       "      <th>minutes_played_home</th>\n",
       "      <th>minutes_played_away</th>\n",
       "      <th>nationality</th>\n",
       "      <th>...</th>\n",
       "      <th>min_per_match</th>\n",
       "      <th>min_per_card_overall</th>\n",
       "      <th>min_per_assist_overall</th>\n",
       "      <th>cards_per_90_overall</th>\n",
       "      <th>rank_in_league_top_attackers</th>\n",
       "      <th>rank_in_league_top_midfielders</th>\n",
       "      <th>rank_in_league_top_defenders</th>\n",
       "      <th>rank_in_club_top_scorer</th>\n",
       "      <th>weekly_wages</th>\n",
       "      <th>market_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2238</th>\n",
       "      <td>Dennis Geiger</td>\n",
       "      <td>21</td>\n",
       "      <td>Bundesliga</td>\n",
       "      <td>2018/2019</td>\n",
       "      <td>Midfielder</td>\n",
       "      <td>Hoffenheim</td>\n",
       "      <td>245</td>\n",
       "      <td>124</td>\n",
       "      <td>121</td>\n",
       "      <td>Germany</td>\n",
       "      <td>...</td>\n",
       "      <td>61</td>\n",
       "      <td>245</td>\n",
       "      <td>245</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>17000</td>\n",
       "      <td>5000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          full_name  age      league     season    position current_club  \\\n",
       "2238  Dennis Geiger   21  Bundesliga  2018/2019  Midfielder   Hoffenheim   \n",
       "\n",
       "      minutes_played_overall  minutes_played_home  minutes_played_away  \\\n",
       "2238                     245                  124                  121   \n",
       "\n",
       "     nationality  ...  min_per_match  min_per_card_overall  \\\n",
       "2238     Germany  ...             61                   245   \n",
       "\n",
       "      min_per_assist_overall  cards_per_90_overall  \\\n",
       "2238                     245                  0.37   \n",
       "\n",
       "      rank_in_league_top_attackers  rank_in_league_top_midfielders  \\\n",
       "2238                             0                               0   \n",
       "\n",
       "      rank_in_league_top_defenders  rank_in_club_top_scorer  weekly_wages  \\\n",
       "2238                             0                       15         17000   \n",
       "\n",
       "      market_value  \n",
       "2238       5000000  \n",
       "\n",
       "[1 rows x 47 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(reg_data.loc[[2238]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_name</th>\n",
       "      <th>age</th>\n",
       "      <th>league</th>\n",
       "      <th>season</th>\n",
       "      <th>position</th>\n",
       "      <th>current_club</th>\n",
       "      <th>minutes_played_overall</th>\n",
       "      <th>minutes_played_home</th>\n",
       "      <th>minutes_played_away</th>\n",
       "      <th>nationality</th>\n",
       "      <th>...</th>\n",
       "      <th>min_per_match</th>\n",
       "      <th>min_per_card_overall</th>\n",
       "      <th>min_per_assist_overall</th>\n",
       "      <th>cards_per_90_overall</th>\n",
       "      <th>rank_in_league_top_attackers</th>\n",
       "      <th>rank_in_league_top_midfielders</th>\n",
       "      <th>rank_in_league_top_defenders</th>\n",
       "      <th>rank_in_club_top_scorer</th>\n",
       "      <th>weekly_wages</th>\n",
       "      <th>market_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1798</th>\n",
       "      <td>Thomas Basila</td>\n",
       "      <td>20</td>\n",
       "      <td>Ligue 1</td>\n",
       "      <td>2018/2019</td>\n",
       "      <td>Defender</td>\n",
       "      <td>Nantes</td>\n",
       "      <td>91</td>\n",
       "      <td>91</td>\n",
       "      <td>0</td>\n",
       "      <td>France</td>\n",
       "      <td>...</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>4000</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          full_name  age   league     season  position current_club  \\\n",
       "1798  Thomas Basila   20  Ligue 1  2018/2019  Defender       Nantes   \n",
       "\n",
       "      minutes_played_overall  minutes_played_home  minutes_played_away  \\\n",
       "1798                      91                   91                    0   \n",
       "\n",
       "     nationality  ...  min_per_match  min_per_card_overall  \\\n",
       "1798      France  ...             46                     0   \n",
       "\n",
       "      min_per_assist_overall  cards_per_90_overall  \\\n",
       "1798                       0                   0.0   \n",
       "\n",
       "      rank_in_league_top_attackers  rank_in_league_top_midfielders  \\\n",
       "1798                             0                               0   \n",
       "\n",
       "      rank_in_league_top_defenders  rank_in_club_top_scorer  weekly_wages  \\\n",
       "1798                             0                       24          4000   \n",
       "\n",
       "      market_value  \n",
       "1798       1100000  \n",
       "\n",
       "[1 rows x 47 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_data.loc[[1798]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
